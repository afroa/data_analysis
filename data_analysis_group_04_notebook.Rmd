---
title: "data_analysis_group_04_notebook"
output: html_document
date: '2022-04-15'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

Deliverable 2 (week 11) - 10 percent: 
Turn in, via GitHub, an updated draft data notebook (.RMD) that :
Loads and cleans the core data set to prepare for analysis. 
Shows basic exploratory analysis to demonstrate an understanding of the dataset, including the number of rows and columns, any obvious limitations or flaws and any reasons why it might not be able to answer the questions you've posed last week.

Here are the things we set out to do in cleaning this data:
- Clean county names to match census 
- Get rid of date created column
- Try to get rid of MOE row

We first tried to clean the data in R. We were successful in filtering by income to remove the MOE data that we did not need. We wanted to flip the rows and columns of the original dataframe so it would be easier to map onto census data later on. But we ran into *significant* hurdles trying to flip them. When we used the t() function -- which we Googled -- it saved it as a matrix instead of a dataframe. When we changed it to a dataframe, it looked like the data was consistent but when we tried to export it to Open Refine to continue cleaning it got very messy (it got rid of all of the county names each time). After two hours, we decided to clean up this data manually in excel. The small scale of the data allowed us to do this because there were so few rows and columns, but in the future we know this is not a sustainable way of cleaning data. (Do you know what caused this to happen? Or ways that we could've gone about this differently?) In excel, we standardized the county names to align with the census data, we transposed the rows and columns and added a "name" column that we can use in future analysis and got rid of excess rows, such as the MOE and "income" rows, and date created column. 

There are 11 columns and 25 rows in our cleaned data set, one row for each Maryland jurisdiction. This data is limited because it only has median income, so we will have to join it to other demographic and census data. We will be using the census data on home ownership. At this point, we do not have any housing data included in our analysis data, so for our next milestone we will work on joining datasets with the one we just cleaned. 

```{r}

library(tidycensus)
library(tidyverse)
library(janitor)
library(dplyr)

#Reading the data into a new dataframe

md_median_household_income <- read.csv("data/md_median_household_income.csv")

# Cleaning the data

clean_rows <- md_median_household_income %>%
  filter(Data == "Income") %>%
  clean_names() %>%
  t() 

flipped_rows <- as.data.frame(clean_rows)
  
flipped_rows %>%
  rename(flipped_rows, V1 = "2010")

#  mutate(COUNTY = toupper(str_remove_all(COUNTY,", West Virginia|County"))) %>%
#  mutate(COUNTY = str_trim(COUNTY,side="both"))

```

```{r}

cleaned_data <- read_csv("data/final_final.csv")

```

